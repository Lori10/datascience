{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How it works ? \n",
    "* An ensemble method is a technique that combines the predictions from multiple machine learning algorithms together to make more accurate predictions than any individual model. Bagging is a model which is composed of different other models which are trained on different subset of the trainig set independently and then the average value of the models will be the final prediction (regression) and the majority vote of the classifiers will be the final prediction (classification).  <br><br>\n",
    "* Random Forest is an extension of bootstrap aggregation (bagging) of decision trees (independent decision trees which means each decision tree of random forest will be trained independently using its own random  bootstrap sample) and can be used for classification and regression problems. If we use resampling without replacement each decision tree will be trained on different random samples from trainig data; in case of resampling with replacement there might be same training examples in different samples. Those samples from training data are selected row- and column wise randomly. Bagging is a general procedure that can be used to reduce the variance for those algorithm that have high variance. An algorithm that has high variance are decision trees, like classification and regression trees. (CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does it make predictions ? \n",
    "First we will get the prediction of the input data from each decision tree and then average the results. In case of regression we take the mean as the final prediction, in case of classification we take the majority vote (mode) from the decision tree predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What Are the Basic Assumption?\n",
    "There are no such assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advantages\n",
    "### Advantages\n",
    "\n",
    "1. Doesn't Overfit; It usually gives Low Bias and Low Variance. It solves the problem of overfitting of Decision Tree.\n",
    "2. No feature scaling required: No feature scaling (standardization and normalization) required in case of Random Forest as it uses DEcision Tree internally\n",
    "3. Suitable for any kind of ML problems (also non linear relationships between X and y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "1. Biased With features having many categories\n",
    "\n",
    "2. Biased in multiclass classification problems towards more frequent classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Whether Feature Scaling is required?\n",
    "No because it uses Decision Tree internally.\n",
    "\n",
    "### 6. Impact of outliers?\n",
    "Robust to Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Why do ensemble techniques typically have higher scores than individual models ?\n",
    "An ensemble is the combination of multiple models  which learn different patterns from the data to create a single prediction. The key idea for making better predictions is that the models should make different errors. That way the errors of one model can bo compensated by th right guesses of the other models and thus the score of the ensemble will be higher. They also prevent overfitting because the model is not trained on the entire dataset but different models are trained on different subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How can Random Forest solve the problem of overfitting that has many ML Estimators like Decision Tree ?\n",
    "Random Forest usually results in low variance because each decision tree is trained on a different random sample and different subset of features and not on the same data, so we will not end up creating a model that fits very well on a particular dataset but train it on different samples and features from these data using decision tree and then try to average the results from each model. Those erros made by one decision tree can be compensated by the right predictions of other decision trees und those results in better final predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of Random Forest\n",
    "* The most important hyperparameters (these hyperparameters have high impact on model performance while others not that much) of Random Forest are :\n",
    "1. n_estimators : number of trees in the forest. <br>\n",
    "   Typical values : 100-1000 <br>\n",
    "   Note : no need to tune this hyperparameter. n_estimators must be relative high to make sure we create a generalized model. No need to tune this hyperparameter. Always increasing n_estimator is not going to deacrease the test score that much. We can set it manually to a relative high value (not too high since the training and prediction time will increase).<br>\n",
    "2. max_depth : maximum number of levels in each decision tree. <br>\n",
    "   High values can lead to overfitting, low values can lead to underfitting. <br>\n",
    "   Typical values : [5, 8, 15, 25, 30, None]. (None means no depth limit; by default max_depth=None)\n",
    "   Note : (no need to tune this hyperparameter since randomforest can handle overfitting by having many decision trees. This parameter should be set to a reasonable amount depending on the number of features of your tree. Set it to a reasonable value. <br>\n",
    "   \n",
    "3. max_features : max number of features considered for splitting a node. <br>\n",
    "   High values can lead to overfitting, low values can lead to underfitting. <br>\n",
    "   Possible values : 'sqrt', 'log2', None. If None max_features=n_features. If 'sqrt' max_features=sqrt(n_features) or      if 'log2' max_features=log2(n_features) can be selected as max features for node splitting.\n",
    "   OR it can take float number 0.2-1<br>\n",
    "4. min_samples_split : min number of data points placed in a node before the node is split. <br>\n",
    "   High values can lead to underfitting, low values can lead to overfitting. <br>\n",
    "   Possible values : # if we have huge dataset : space = [2, 50, 200, 400, 700, 900]; If we have small dataset : space = [2, 25, 50, 100, 200] <br>\n",
    "5. min_samples_leaf : min number of data points allowed in a leaf node. <br> \n",
    "   Low value can lead to overfitting, large values can lead to underfitting. <br>\n",
    "   Possible values : if we have huge dataset space = [2, 50, 200, 400, 600] ; if we have small dataset space = [1, 25, 50, 100, 300] <br>\n",
    "   \n",
    "6. criterion : criteria based on which the feature impurities will be measured when finding best feature to split the data. <br>\n",
    "   Possible values : 'gini' or 'entropy' for Classification and 'mae' or 'mse' for Regression. \n",
    "   Note : For Regression, the general rule is to take MSE if you donâ€™t have many outliers in your data, as it penalises highly    those observations that are far away from the mean. For classification, the thing is a bit more tricky. We have to calculate a measure of impurity with either Gini or Entropy, which can result in a different split sometimes.\n",
    "7. max_leaf_nodes : max nr of data points a leaf node can have. <br>\n",
    "   High value can lead to  underfitting, low value can lead to overfitting. <br>\n",
    "   Typical values : 2-60 <br>\n",
    "   Note : no need to tune this hyperparameter. max_leaf_nodes=None means there is no limit in the number of lead nodes.<br>\n",
    "8. bootstrap = method for sampling data points (with or without replacement). True if we want to use boostrap sampling or False if we want to use whole data for each decision tree.\n",
    "   Typical values : True or False. <br>\n",
    "   Note : No need to tune bootstrap (set it always to True)\n",
    "    \n",
    "5. class_weight (dictionary) : this hyperparamter is used to handle imbalanced data. If we set high weight in the loss function means it leads to higher error in the loss/cost function when we incorrectly classify a class that has low frequency (we penalize the class with low frequncy more) <br>\n",
    "    Note : Set low weight for class that has high frequency and high weight for class that has low frequency.\n",
    "    For example if 0 has high frequency and 1 very low frequency we should set {0:0.1, 1:0.9}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
