{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Learn Machine Learning Algorithms For Interviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How it works ?\n",
    "A decision tree is one of the popular as well as powerful tools which is used for prediction and classification of the data or an event. It is like a flowchart but having a structure of a tree. \n",
    "* First we choose a metric based on which we compute the impurity of the split (entropy, gini, missclassification rate). Entropy \n",
    "shows the impurity of the split (how well a feature splits the data; the idea is to increase the certainty of Y after the split ; higher the gini/entropy the higher impurity -> similar frequency of all class in Y; the lower gini/entropy the lower the impurity -> high frequncy of one class and low frequency of other classes; special cases: entropy,gini=0 we have only one class in Y; entropy=gini=highest value we have uniform distribution of each classes). For each feature and for each split inside that feature we compute the Information Gain = Entropy before split - Entropy after the split. Entropy after the split is equal to the sum of entropys of each subset multiplied by the relative frequencies (we assign a weight to each subset based on their size). We keep splitting the nodes until we reach leaf nodes where we have only one class (max_depth=None) or until a condition is met (for example hyperparameters like max_leaf_nodes or max_leaf_nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How Decision Tree splits categorical and continuous features ?\n",
    "\n",
    "Decision Tree from Sklearn uses only binary splits. This if we have a categorical ordinal feature X which takes the distinct values 1,2,3 we would create 3 splits on this features : X==\"1\" (X is 1 or (1 or 2), X==\"2\", X==\"3\". If we have a continous feature X which takes the distinct sorted values : 1.1, 2.3, 3.2 we would create the following splits X==(1.1 + 2.3)/2, X==(2.3+3.2)/2. So we take as the threshold of the split the middle of two values in a sequence. This will lead to too many splits if we have very large dataset (overfitting). Solution : use discretization to create bins (discretization based on quantiles instead discretization using same interval size in order the have the same amount of data on each interval since the distribution of our data is usually not uniform) in the continous feature and use as a threshold the boundaries of the bins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is the disadvantage of having features with very large distinct values (categories) and how to handle it ?\n",
    "\n",
    "If we have features with too many distinct values for example (extrem case) we have the feature data where all values are distinct. The decision tree would choose this feature to be on the best split since for each distinct value (category/date) we have only 1  single class (entropy/gini=0 after the split) and the decision tree would already be built using this single feature. This is not a good decision since it may overfit and in general leads to too many split because of the large nr of categories. <br>\n",
    "Solution : instead of computing the Information Gain we build a new metric called Gain Ratio = Information Gain / Instrinct Information. Instrinct Information is the entropy of the feature X which would be high if in X are too many distinct values and small if not. This would case the Gain Ratio of features with many categories to be smaller and we choose the feature with highest Gain Ratio to be part of the split. Still the approach can lead to choosing features which have too many categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How do Regression Trees (CART) work ?\n",
    "\n",
    "* We use Regression Trees if the target variable is continuous. Instead of computing the gini/entropy we compute the Variance of Y on the tree nodes. The idea behind CART is to split if the Variance of Y after the split is lower than before split (which means the certainty of the output Y value is greater if we have low variance. We try to minimize the following equation. We aim to minimze the squared difference of each subset of the split (mean is the best estimation of c) so we end up computing the variance. As a result we want to minimize the Sum of the Variances of y after the split and we choose the feature and the threshold which leads to such a minimum. After the Tree is built, in the leaf nodes we compute the mean of that subset as the final output.\n",
    "\n",
    "<img src=\"images/cart.png\" width=\"600\" height=\"440\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What Are the Basic Assumption?\n",
    "There are no such assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Advantages and Disadvantages\n",
    "### Advantages\n",
    "\n",
    "1. Clear Visualization: The algorithm is simple to understand, interpret and visualize as the idea is mostly used in our daily lives. Output of a Decision Tree can be easily interpreted by humans.\n",
    "\n",
    "2. Simple and easy to understand: Decision Tree looks like simple if-else statements which are very easy to understand.\n",
    "\n",
    "3. Decision Tree can handle both continuous and categorical variables.\n",
    "\n",
    "4. Handles non-linear features (features that does not have linear relationship with target feature) efficiently: Non linear parameters don't affect the performance of a Decision Tree unlike curve based algorithms. So, if there is high non-linearity between the independent variables, Decision Trees may outperform as compared to other curve based algorithms like LinearRegression.\n",
    "\n",
    "5. Less Training Period: Training period is less as compared to Random Forest because it generates only one tree unlike forest of trees in the Random Forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "\n",
    "1. Overfitting (High Variance): This is the main problem of the Decision Tree. It generally leads to overfitting of the data which ultimately leads to wrong predictions. In order to fit the data (except the normal data even noisy data), it keeps generating new nodes and ultimately the tree becomes too complex to interpret. In this way, it loses its generalization capabilities. It performs very well on the trained data but starts making a lot of mistakes on the unseen data. \n",
    "\n",
    "3. Unstable: Adding a new data point can lead to re-generation of the overall tree and all nodes need to be recalculated and recreated. \n",
    "\n",
    "4. Need for large datasets: If data size is smalll, then one single tree may grow complex and lead to overfitting. So in this case, we should use Random Forest instead of a single Decision Tree.\n",
    "\n",
    "5. Pure greedy approach : in each node we find the best split it leads to local minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Whether Feature Scaling is required?\n",
    "No feature scaling required: No feature scaling (standardization and normalization) required in case of Decision Tree as it uses rule based approach (splits the data based on feature values) instead of distance calculation.\n",
    "\n",
    "### 5. Impact of outliers?\n",
    "It is not sensitive to outliers. Boosted Tree methods should be fairly robust to outliers in the input features since the base learners are tree splits. For example, if the split is x > 3 then 5 and 5,000,000 are treated the same.This may or may not be a good thing, but that's a different question. <br>\n",
    "Since, extreme values or outliers, never cause much reduction in RSS, they are never involved in split. Hence, tree based methods are insensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Impace of Missing Values ?\n",
    "\n",
    "Decision Trees can handle missing values automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Overfitting and Underfitting\n",
    "The big advantage of Decision Tree is that it usually overfits to the training data (in case we dont specify a limited nr of max depth) because it always uses data splits and it may also fit very well to the outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Why does Decision Tree overfit ?\n",
    "In decision trees, over-fitting occurs when the tree is designed so as to perfectly (it has too many nodes) fit all samples in the training data set. Thus it ends up with branches with strict rules of sparse data. It may create branches that predict perfectly even single training examples or very few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How to avoid overfitting in Decision Trees ? \n",
    "1. Selection right hyperparameters that can handle overfitting. For example reducing the max_depth to get a smaller tree (not complex). Note : Like any other hyperparamter we must find the best value using cross validation.\n",
    "2. Pruning. The performance of a tree can be further increased by pruning. It involves removing the branches that make use of features having low importance. This way, we reduce the complexity of tree by removing nodes from the tree (make it less complex), and thus increasing its predictive power by reducing overfitting.  \n",
    "\n",
    "Two types of Pruning : <br>\n",
    "A) <b>Post-pruning</b>, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed. Post pruning can be performed in two ways : 1) reduced error pruning, 2) cost complexity pruning. <br> \n",
    "1) <b>Reduced Error Pruning : </b><br>\n",
    "Cross-validation set of data is used to check the effect of pruning  whether expanding a node will make an improvement or not. If any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node. For example : The simplest method of pruning starts at leaves and removes each node with most popular class in that leaf, this change is kept if it doesn't deteriorate accuracy. While somewhat naive, reduced error pruning has the advantage of simplicity and speed. <br>\n",
    "2) <b>Cost Complexity Pruning : </b> <br>\n",
    "\n",
    "* We define the cost complexity R_alpha(T) = miss_classification_rate + nr of lead nodes. Miss_classification_rate of a tree is the sum of miss_classification_rate of its leaf nodes. We want miss_classification_rate to be small (like gini,entropy); miss_classication_rate = 1 - argmax(p_each_class). This means trees with too many leaf nodes will be penalized and have a high error R_alpha(T). The greater the alpha => the more we penalize those tree => we end up choosing a simpler tree => underfitting. The lower alpha => overfitting. Our objective is for given a specific value of alpha to choose the decision tree which minimized R_alpha(T). For different alphas we end up choosing different optimal trees.\n",
    "* We set alpha=0. We first start with the biggest tree T(max) (pre-pruning). R_alpha(t) = R(t) + alpha; R_alpha(T_t) = R(T_t) + alpha * nr_leaf_nodes_of_Tt). For alpha=0 we get as optimal tree T(1)=T(max) since we try to minize R_0(T) = R(T) + 0=R(T) for all other trees R_0(T) > R_O(T_max). \n",
    "* Lets look at the R_alpha(t) and R_alpha(T_t). t is a node(subset) of the tree and T_t is the branch which begins at the node t and continues to the end of the tree. R_alpha(t) = R(t) + alpha * nr_leaf_nodes_of_t; R_alpha(T_t) = R(T_t) + alpha * nr_leaf_nodes_of_Tt. For alpha=0, R_0(t) = R(t) > R_0(T_t) = R(T_t) + alpha * nr_leaf_nodes_of_T_t . If we increase alpha a bit these condition will be met until we each a particular valua of alpha_1 since the right side increases more than left side. Now we define a function g(t) for each node and choose the node t which minimizes this function (we want to minimize the R_alpha(T)) and compute the second tree T2 = T1 - T_t.\n",
    "\n",
    "<img src=\"images/dt1.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "<img src=\"images/dt2.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "<img src=\"images/dt3.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "<img src=\"images/dt4.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "<img src=\"images/dt5.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "<img src=\"images/dt6.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "<img src=\"images/dt7.png\" width=\"600\" height=\"440\">\n",
    "\n",
    "\n",
    "\n",
    "B) <b>Pre-pruning</b>, also known as forward pruning, stops the non-significant branches from generating. After computing the information gain for each feature it checks whether the relationship between the feature x and y is statistically significant using different chi-squared test like test of independece, test of homogenity, goodness of fit. Only statistically significant branches will be split, otherwise we do not perform the split. \n",
    " <br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Entropy and Information Gain\n",
    "<b>Entropy</b> is the measure of randomness in the data or the amount of unpredictability in a random variable. In other words, it gives the impurity present in the dataset (measures the impurity of a particular independet feature with respect to target feature).\n",
    "A region/split is clean (low entropy) when both branches  contain data with the same labels and random if there is a mixture of labels present (high entropy) in each branch. When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop. <br>\n",
    "\n",
    "For example if entropy=0 (lowest value) we would be have 2 branches where each branch has only 1 label/class (best case). Entropy = 0.5 (highest_value) would result in 2 branches where in each branch we have same amout of examples for each class/label (worst case).\n",
    "\n",
    "<b>Information gain</b> calculates the decrease in entropy after splitting a node. It is the difference between entropies before and after the split. The more the information gain, the more entropy is removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Gini Impurity\n",
    "<b>Gini Impurity</b> is very similar to Entropy. It also measures the randomness of the data or the amount of unpredictability in a random variable.\n",
    "n other words, it gives the impurity present in the dataset (measures the impurity of a particular independet feature with respect to target feature).\n",
    "A region/split is clean (low gini) when both branches  contain data with the same labels and random if there is a mixture of labels present (high gini) in each branch. When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the gini i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into gini reduction, we try to split based on a different condition, or we stop. <br>\n",
    "For example if gini=0 (lowest value) we would be have 2 branches where each branch has only 1 label/class (best case). Gini = 1 (highest_value) would result in 2 branches where in each branch we have same amout of examples for each class/label (worst case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Diference between Gini and Entropy\n",
    "Gini and Entropy are very similar and they measure the same thing (impurity of a feature) but they are mathematically calculated in different manners. Most of people use gini since it less computationally expensive. But in some cases gini and entropy results in different splits so it would be good idea to include those values in hyperparameter tuning if we want to achieve highest possible performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters of Decision Tree\n",
    "* A node with only observations of class A is 100% pure according to both, Gini and entropy. \n",
    "* A node with 10 observations of class A, and 10 of class B is 100% impure according to both, Gini and entropy.\n",
    "* A node with 3 observations of class A and 1 of class B is ether 75% or 81% impure, depending if we use Gini or Entropy respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most important hyperparameters (these hyperparameters have high impact on model performance while others not that much) of DecisionTree are:\n",
    "1. max_depth : maximum depth of the tree. <br>\n",
    "   High values can lead to overfitting, low values can lead to underfitting <br>\n",
    "   Typical values : 1-32.\n",
    "2. max_features : The number of features to consider when looking for the best split. <br>\n",
    "   High values can lead to overfitting, low values can lead to underfitting. <br>\n",
    "   Possible values : <br> \n",
    "   If “auto”, then max_features=sqrt(n_features).\n",
    "    If “sqrt”, then max_features=sqrt(n_features).\n",
    "    If “log2”, then max_features=log2(n_features).\n",
    "    If None, then max_features=n_features. If float float, then max_features is a fraction and int(max_features * n_features) features are considered at each split. If int, then consider max_features features at each split.\n",
    "3. min_samples_leaf : The minimum number of samples required to be at a leaf node <br>\n",
    "   High values can lead to underfitting, low values can lead to overfitting. <br>\n",
    "   Possible values : <br>\n",
    "   If int, then consider min_samples_leaf as the minimum number.\n",
    "If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node. <br>\n",
    "   Typical values : \n",
    "4. min_samples_split : The minimum number of samples required to split an internal node. <br>\n",
    "   High values can lead to underfitting, low values can lead to overfitting. <br>\n",
    "   Typical values : \n",
    "   \n",
    "5. ccp_alpha : Complexity parameter used for Minimal Cost-Complexity Pruning. The subtree with the largest cost complexity that is smaller than ccp_alpha will be chosen. By default, no pruning is performed.\n",
    "   \n",
    "5. class_weight (dictionary) : this hyperparamter is used to handle imbalanced data. If we set high weight in the loss function means it leads to higher error in the loss/cost function when we incorrectly classify a class that has low frequency (we penalize the class with low frequncy more) <br>\n",
    "    Note : Set low weight for class that has high frequency and high weight for class that has low frequency.\n",
    "    For example if 0 has high frequency and 1 very low frequency we should set {0:0.1, 1:0.9}.\n",
    "5. splitter : The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split (best feature to split) and “random” to choose the best random split. <br>\n",
    "  'Best' can lead to overfitting, 'random' can lead to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
