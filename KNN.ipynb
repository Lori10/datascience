{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How it works ? \n",
    "A) KNN finds the distance of each point from test set with all points from training set. \n",
    "B) It select k training points that are nearest to the point from test set\n",
    "C) In case of classification it calculates it calculates the probabilities of each class and select the class with highest probability; in case of Regression ti calculates the average of these k points. <br>\n",
    "\n",
    "To calculate the distances it can use Euclidean Distance, Mannhatan Distance, Hamming Distance.<br>\n",
    "* Euclidean distance is the most commonly used method to calculate the distance between two points.\n",
    "<img src=\"images/5.png\" width=\"\">\n",
    "* Mannhatan distance represents the sum of the absolute differences between the opposite values in vectors.\n",
    "<img src=\"images/8.png\" width=\"\">\n",
    "* Hamming distance is a distance metric that measures the number of mismatches between two vectors. It is mostly used in the case of categorical data. Generally, if we have features as categorical data then we consider the difference to be 0 if both the values are the same and the difference is 1 if both the values are different.\n",
    "<img src=\"images/7.png\" width=\"\"> <br><br>\n",
    "\n",
    "#### Weighted Nearest Neighbor\n",
    "Weighted k-NN is a modified version of KNN.  In weighted kNN, the nearest k points are given a weight using a function called as the kernel function. The intuition behind weighted kNN, is to give more weight to the points which are nearby and less weight to the points which are farther away. Any function can be used as a kernel function for the weighted knn classifier whose value decreases as the distance increases. Sometimes rest of data points are assigned a weight of 0. For example We might face cases where the model does not classify test points in a proper way because of some outliers that might be part of k nearest points. So what we do is that we assign weights to the points that are not outliers to have better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What Are the assumption?\n",
    "There are no such assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Advantages and Disadvantages \n",
    "### Advantages\n",
    "1. It can be used for both regression and classification problems.\n",
    "2. It is very simple and easy to implement.\n",
    "3. Mathematics behind the algorithm is easy to understand.\n",
    "4. There is no need to create/train the model so there is not much time cost in training phase.\n",
    "5. KNN doesn't make any assumption for the distribution of the given data.\n",
    "\n",
    "### Disadvantages\n",
    "1. Finding the optimum value of ‘k’\n",
    "2. It takes a lot of time when it comes to predict test examples/to compute the distance between each test sample and all training samples. \n",
    "3. Since the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again (computing distances with all training samples). We need to store the whole training set for every test set, so it requires a lot of space.\n",
    "5. It is not suitable for high dimensional data because computing the distances between points of high dimensions is very expensive and costly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Whether Feature Scaling is required?\n",
    "Yes because the distance calculation done in KNN uses feature values. When the one feature values are large than other, that feature will dominate the distance hence the outcome of the KNN. For example if we have feature x1 in range 1-10 and feature x2 in range 10k-100k. If we calculate the euclidean distance between 2 particular points it will be a huge value because of the high magnitude of feature x2 even though they might be similar points. This will impact the performance of all distance based model as it will give higher weightage/distance to variables which have higher magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Impact of outliers?\n",
    "It is sensitive to outliers if value of k is too small. Suppose k=1 and this nearest points is an outlier so we would assign to the test data point the class of an outlier which would not be ok."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Impact of Missing Values ?\n",
    "\n",
    "KNN can not handle missing values automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to select the value of k (nr of nearest neighbors) ?\n",
    "The value of k affects the k-NN classifier drastically. The flexibility of the model decreases with the increase of ‘k’.  With lower value of ‘k’ variance is high and bias is low but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing. This means with very low values of ‘k’ there is a chance of algorithm overfitting the data whereas with very high value of ‘k’ there is a chance of underfitting so we must a find a proper value to avoid over- and underfitting. If k is too small, the algorithm would be more sensitive to outliers. If k is too large, then the neighborhood may include too many points from other classes. Like any other hyperparameter we must use cross validation to find the best value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning of KNN \n",
    "* KNN has only few hyperparameters to tune. The most important hyperparameters (these hyperparameters have high impact on model performance while others not that much) of KNN are:\n",
    "1. n_neighbors : Number of neighbors to use. <br>\n",
    "   High values of n_neighbors can lead to underfitting, low values can lead to overfitting. <br>\n",
    "   Typical values : 1-100\n",
    "2. weights : weight function used in prediction. <br>\n",
    "  'uniform' : uniform weights. All points in each neighborhood are weighted equally. <br>\n",
    "  ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a    greater influence than neighbors which are further away.<br>\n",
    "  callable : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the  weights. <br>\n",
    "   'Distance' can lead usually to overfitting, 'uniform' can generalize much better. <br>\n",
    "   Note : no need to tune this hyperparameter. <br>\n",
    "   Possible values : {‘uniform’, ‘distance’} or callable\n",
    "   \n",
    "   \n",
    "   Other hyperparameter : \n",
    "1. leaf_size : Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the    memory required to store the tree. <br>\n",
    "   Note : no need to tune this hyperparameter. <br>\n",
    "   Typical values : 1-40\n",
    "   \n",
    "2. 'metric' : the distance metric to use for the tree.\n",
    "   Possible values : ['minkowski','euclidean','manhattan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
