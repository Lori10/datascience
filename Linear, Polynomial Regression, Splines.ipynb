{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How it works ?\n",
    "* Using Linear Regression algorithm we try to find line (in case of 1d/1feature data) / hyperplane (in case of more than 2d data) line that best fits our dataset. In otherword we want to find parameters theta_0 ... theta_p where p is the nr of features so that the cost function (mean squared error) is minimized. \n",
    "* The technique used to find these optimized parameters is called Ordinary Least Squared (normal equation of OLS) where the parameters that minimize the cost function theta = inverse(t(X) * X) * t(X) * Y (some matrix multiplications). By multiplying all these matrices we perform the following steps : set first derivative to 0 or partial derivative vector for each parameter since we have more than 1 feature) and second derivative to be positive (if we have only 1 feature) or hessematrix to be positiv definint if we have many features).\n",
    "* y_pred = theta_0 + theta_1 * x_1 + ... theta_p * x_p + residual is the real but unknown model which we aim to estimate accuractly. \n",
    "  y_pred = estimated_theta_0 + estimated_theta_1 * x_1 + ... estimated_theta_p * x_p + estimated_residual is the estimated model using OLS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does it make predictions and interpretation ? \n",
    "* pred = theta0*x0 + theta1*x1 + theta2*x2 + ... + theta_n*x_n where theta0, theta1 ... thetan are the model parameters/coefficients and x0,x1 ... xn are the independet features. Features with higher weights will usually have higher affect/importance on making prediction. \n",
    "* Example 1 : Suppose we have the model y = 2.14 + 2*x + theta2 * feature2 ...   <br>\n",
    "Model Interpretation : If height increases by 1 and we hold all other features constant, weight increases by 2. <br>\n",
    "Example 2 : we have the model log(weight) = 2.14 + 0.025 * height + theta2 * feature2 ... <br>\n",
    "Model Interpretation : If theta1 = 0.5, an increase of one unit in height is associated with a 0.025*100 % = 2.5% percent change in weight. <br>\n",
    "Example 3 : We have the model : weight = 3.94 + 1.16*log(height) + theta2 * feature2 ... <br>\n",
    "Model Interpretation : theta1 = 1.16 and we would say that a 1 increase in height is associated with an \n",
    "increase of theta1 *100 = 116 in weight. <br>\n",
    "Example 3 : log(weight) = 1.69 + 0.11log(height) + theta2 * feature2 ... <br>\n",
    "Model Interpretation :  theta1 = 0.11 and we would say that a one-percent increase in height is associated with about a 0.11 percent change in weight.\n",
    "\n",
    "Reference : https://cscu.cornell.edu/wp-content/uploads/83_logv.pdf#:~:text=Interpreting%20parameter%20estimates%20in%20a%20linear%20regression%20when,the%20response%20variable%2C%20holding%20all%20other%20predictors%20constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What Are the Assumptions ? \n",
    "\n",
    "1. Linearity: The relationship between features X and the mean of Y is linear. (apart from the linear relationship there are other types of relationship that exists between 2 variables like quadratic or cubic). <br> To check for this asssumption we can visualize a scatterplot of each features with the target; perform t-test for the regression coefficients thetas, compute confidence intervalls for the regression coefficients theta, compute the correlation coefficient of each feature with the target.\n",
    "2. Independence of features (No Multicolleniarity): Features x1 ... x_p must not be not highly correlated. This is often the case in real time datasets. <br> To check for this assumption we can either plot the correlation heatmap or compute Variance Inflaction Factor for each features. (check Feature Selection)\n",
    "3. E(residuals) = 0 (expected value) and VAR(residuals) = sigma^2. Residuals are unlike labels and features random variables. Excpected value of residuals = 0 means that the most data points are very closely to our fit line which basically means the model is doing well. Variance of residuals is the same (sigma^2) is called Homoskedasticity. No matter what the features x are the residuals have the same variance (variance of residuals does not depend on the value of x). <br> To check for this assumption we can plot the each feature x_i vs residuals or predicted_values vs residuals and see how the variance of residuals behaves and what is the mean (estimation of expected value of residuals).\n",
    "4. Residuals (error terms) must not be correlated. To check for this assumption we can plot the residuals vs index and check if there is any trend or correlation.\n",
    "5. Rang(X) = p+1 where X is Design Matrix (feature matrix) which means all features must be linear independent , Corr(X_i, X_j) != 1. We rarely see features that are perfectly correlated so this assumption is usually satisfied. Otherwise we would have several parameters that minimize the cost function.\n",
    "5. Standard Normal Distribution of Residuals. (This is an assumption only when we perform t-test and compute confidence intervalls about the regression coefficients theta_0 .. theta_p and not when we perform OLS). As a result predicted values would be normally distributed and the test_statistic of t-test would be t distributed. Since we know the distribution of test_statistic we can perform a t-test where H_0 : theta_i = 0, H_1 : theta_i != 0. <br>\n",
    "To check for this assumption we can visualize QQPlot of residuals or histogram of residuals or perform shapirko wilk test which tests whether a distribution is normal.\n",
    "\n",
    "\n",
    "\n",
    "* When training Linear Regression if any of this asssumptions is not satisfied that would impact the model performance negatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advantages and Disadvantages\n",
    "### Advantages\n",
    "1. Linear regression performs exceptionally well for linearly separable data (features have linear relationship with target)\n",
    "2. Easy to implement and train the model\n",
    "3. It can handle overfitting using dimensionlity reduction techniques, regularization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "1. Sometimes Lot of Feature Engineering Is required\n",
    "2. If the independent features are correlated it may affect performance\n",
    "3. It is often quite prone to noise and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Whether Feature Scaling is required?\n",
    "No, feature scaling does not affect the performance of a linear regression model as long as we do not perform linear regression with regularization. In penalized linear regression, scaling does affect it. Upscaled variables become more penalized, i.e. have coefficients shrunk more towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 . Impact of Missing Values?\n",
    "It is sensitive to missing values so we must handle them before training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Impact of outliers ?\n",
    "It is sensitive to outliers. <br>\n",
    "A model is sensitive to outliers : this means a single bad prediction would ruin the entire model's predicting abilities because the model is going to penalize too high the cost function (high increase in cost function) because of the errors caused by outliers. Se we would end tup choosing the model with lowest cost function which means the model has best fitted to the outliers. <br>\n",
    "A model is robust to outliers : this means erros caused by outliers are not going to penalize (increase so much) the cost function will not get affected too much which means the model is going to ignore erros from outliers and will end up choosing a model which fits the other part of dataset (exluding outliers) <br>\n",
    "<img src='images/img20.png' width=\"700\" height=\"700\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Overfitting And Underfitting\n",
    "Linear Regression model can overfit to the training data which means it will fit very well the training data (even noisy data). Linear Regression can avoid overfitting using reguluarization techniques (L1, L2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B)  Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How does it make prediction ?\n",
    "Y = β0 + β1*X + β2*X_squared + … + βhXh + ε (we take the square, cube or any other degree of some particular features) to create polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What are the assumptions ? \n",
    "Nonlinearity : Polynomial Regression assumes that the relation between independent and dependent feature is not linear.\n",
    "This means we should use Polynomial Regression if the relationship between independet and depenent is not linear (simple) but more complex or if we fit a simple linear regression model to a dataset and the R2 value of the model is quite low, this could be an indication that the relationship between the predictor and response variable is more complex than just a simple linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. How to choose the degree of the polynomial (hyperparameter) ?\n",
    "We can either use k-fold cross validation or ANOVA. Usually at most a degree of 3 is used. <br>\n",
    "A) Usually we fit several different models with different values of h and perform k-fold cross-validation to determine which model produces the lowest test mean squared error (MSE). There exists a bias-variance tradeoff when using polynomial regression. As we increase the degree of the polynomial, the bias decreases (as the model becomes more flexible) but the variance increases (we create a more complex model).\n",
    "As with all machine learning models, we must find an optimal tradeoff between bias and variance.\n",
    "In most cases it helps to increase the degree of the polynomial to an extent, but beyond a certain value the model begins to fit the noise of the data and the test MSE begins to decrease. <br>\n",
    "B) We could perform statistical test ANOVA. By increasing the degree of the polynomial we check whether the current polynomial is more statistically significant (lower p-value) than the previous model which has a lower degree. If has lower p-value it means that this model is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advantages and Disadvantages \n",
    "<b>Advantages :</b> <br>\n",
    "It captures complex (non linear relationships) in features.\n",
    "\n",
    "<b>Disadvantages :</b> <br>\n",
    "1. Polynomial Regression imposes a global structure of the relationship for example : in cubic polynomial regression the relationship between x and y is cubic for the entire span(values) of x. In x we usually have different patterns and relationships.\n",
    "2. It  can capture complex relationship using higher polynomial degree but compared to Neural Networks which uses activation function to capture non-linear relationship, Regression just sums up the products (coefficient * feature). Thats where it fails and is outperformed by NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Example of Polynomial Regression \n",
    "\n",
    "If we want to predict the salary based on Age a cubic (degree of 3) polynomial regression would fit the data better than the linear regression. For young ages the salary is low, for mid ages the salary becomes higher and for old people the salary decreases again. This type of relationship of age and salary is cubic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C) Splines (Local Linear Regression)\n",
    "* To avoid the global structure problem of Polynomial Regression we use splines which means we fit a polynomial regression for different ranges of x which is usually a better model than the standard polynomial regression. We should determine how many thresholds to use, the intervalls (ranges) and the degree of the polynomial regression.\n",
    "* Natural Splines is a specific type of Splies which in the boundary regions of x is linear and not polynomial like normal splines which leads to a better generalization.\n",
    "* Splines usually outperform the polynomial regression model since it fits different models for different patterns on x."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
