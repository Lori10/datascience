{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How does Gradient Descent work ?\n",
    "\n",
    "The goal of Gradient Descent : find theta0 ... thetan so that J(theta0, .. thetan) -> cost function is minimized. <br>\n",
    "\n",
    "1. Initialize weights and bias for each layer. <br>\n",
    "\n",
    "repeat for a specific nr of epochs : { <br>\n",
    "\n",
    "2. Perform forward propagation. Using forward propagation we pass a particular training sample (in case of mini batch GD we pass the batch, in case of batch gradient descent we pass the entire training data; in case of stochastic gradient descent we pass only 1 single example) to the model and get the predictions. Calculate the cost function. (this is done just to check if the cost function decreases in each step of gradient descent)\n",
    "2. Perform backward propagation. Using backward propagation we find the partial derivative of cost function with respect to each parameter/weight using that batch sample based on the cost function for the batch. (During backward propagation the model learns).\n",
    "3. Update each parameter/(weights and bias) using the partial derivative calculated in step 4 and alpha. Update Equation : <br>\n",
    "For i in [0, 1, ..._n] { <br>\n",
    "    theta_i (new theta) := theta_i (old theta) - alpha * d(J(theta0, theta1, ..., thetan) / d(theta_i)\n",
    "} <br>\n",
    "} <br>\n",
    "\n",
    "\n",
    "Calculating the derivative of cost function d(J(theta0, theta1, ..., thetan) / d(theta_i). Note : In this case we are using the cost function of Linear Regression and not Neural Networks since the cost function of Neural Networks is much more complicated. \n",
    "Notations : d -> single training example, D -> entire training set, od -> hypothesis, td -> y labels, w -> parameters thetas\n",
    "<img src='images/GradientDescent_Equation.jpg' width=\"400\" height=\"400\"> <br>\n",
    "\n",
    "Note : Logistic Regression use Gradient Descent to minimize the cost function just as Neural Networks. The cost function of them is not the same. The Mean Squared Error CF of Linear Regression and Likelihood Cost Function of Logistic Regression are convex which means they only have 1 minimum (global minimum). On the other side, the cost function of Neural Network is not convex which means it has several minimums (1 global and the other are called local minimum). The optimization problem at NN is more difficult and complicated. It depends on at which point we start (how we initialize the parameters of NN) to figure out which minimumum (local or global) are we trying to reach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How does Forward, Backward Propagation and Update Equation work ?\n",
    "\n",
    "<img src='images/nn1.jpg' width=\"600\" height=\"600\"> <br>\n",
    "<img src='images/nn2.jpg' width=\"600\" height=\"600\"> <br>\n",
    "<img src='images/nn3.jpg' width=\"600\" height=\"600\"> <br>\n",
    "<img src='images/nn4.jpg' width=\"600\" height=\"600\"> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What are the RNN Neural Network Architectures according to the input and output size ?\n",
    "One-to-One : Example -> neural network that predicts whether to approave a loan based on financial information. <br>\n",
    "Many-to-One : Example -> use tweets as input to predict the sentiment (positive or negative). <br>\n",
    "One-to-Many : Exampel -> use images as input to give a description about it as output. <br>\n",
    "Many-to-Many (input and output have different length) -> translate a text from one language into another one. This type of Architechture is called Seq2Seq (sequence to sequence) <br>\n",
    "Many-to-Many (input and output have same length) -> names entity recognition\n",
    "\n",
    "<img src='images/img2.png' width=\"600\" height=\"600\"> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What are some typical problems of Neural Networks regarding the Gradient Descent ?\n",
    "\n",
    "1. Vanishing Gradient Problem \n",
    "2. Exploding Gradient Problem <br>\n",
    "<b>What is the Exploding and Vanishing Gradient Problem?</b>\n",
    "* In a network of n hidden layers, n derivatives will be multiplied together (derivative of activation function of each layer). If the derivatives are large then the gradient will increase exponentially as we propagate down the model until they eventually explode (increases), and this is what we call the problem of exploding gradient (the gradient of deeper layer will be small while the gradient of layers close to input layer will be higer which means layer close to input layers are being well trained while depper layers are getting training a little bit). As a result we may miss the local minima if the gradient explodes (is too high).Alternatively, if the derivatives are small then the gradient will decrease exponentially as we propagate through the model until it eventually vanishes (decreases), and this is the vanishing gradient problem. This means layers close to the input layers have a very small gradient (being trained a bit) while deeper layers have much higher gradient (being trained more).\n",
    "* We usually face the vanishing gradient problem either in the beginnig of training or end. In the beginning the NN is not trained at all. It is not able to yield good results. In the end of the training the NN is already trained but since the gradients are too small (gradient of layers which are close to INPUT layer) those layers are not going to be trained anymore. This may also lead to bad results from the NN since training of some layers is not done properly.\n",
    "\n",
    "<b>What leads to Vanishing Gradient Problem ? </b> <br>\n",
    "A) Softmax can lead to Vanishing Gradient Problem. Using Sotftmax the derivate of this activation function is very low (0-0.25). Multiplication of many numbers lower than 1 leads to a very a low gradient value. Relu was introduced to solve the problem of vanishing gradient since its derivative is not very low. <br>\n",
    "B) A neural network with too many layers. Since we have too many layers we would have to multiply too many derivates of activation functions for each layer (which are values lower than 1). This may lead to a very small gradient => very small weight update => slow training. Especially earlier layers do not learn at all since the gradient of these layers is a result of multiplication of gradient of later layers. <br>\n",
    "<b>What leads to Exploding Gradient Problem ? </b><br>\n",
    "A) Bad Initialization can lead to Exploding Gradients. If we initialize a neural network with unappropriate weights the cost would be huge. Since the loss is huge now the gradients can accumulate during an update and result in very large gradients which eventually results in large updates to the network weights and leads to an unstable network and may not converteg to optimum. The parameters can sometimes become so large that they overflow and result in NaN values. <br>\n",
    "B) A neural network with too many layers. Since we have too many layers we would have to multiply too many derivates of activation functions for each layer which will become greater and greater and thus result in huge model update.\n",
    "C) Relu can also lead to Exploding Gradient if weights are huge. Thats why weights should be initialized with small values. <br>\n",
    "\n",
    "<b>How to check if we have Exploding Gradient Problem in our NN ?</b> <br>\n",
    "A) Instable network which means very different losses sometimes is too large sometimes too small. The model is not learning much on the training data therefore resulting in a poor loss. <br>\n",
    "B) Model weights grow exponentially and become very large when training the model. <br>\n",
    "C) The model weights may become NaN during training.\n",
    "\n",
    "<b>How to check if we have Vanishing Gradient Problem in our NN ?</b> <br>\n",
    "A) The model will improve very slowly/loss will decrease slowly during the training phase and it is also possible that training stops very early, meaning that any further training does not improve the model. <br>\n",
    "B) The parameters of the higher layers change significantly whereas the parameters of lower layers would not change much. <br> \n",
    "C) The model weights may become 0 during training (For example using Relu, its derivative can be exactly 0 => entire gradient to be 0)\n",
    "\n",
    "<b>How to solve the problem of Exploding and Vanishing Gradient  ?</b> <br>\n",
    "A) Reduce amount of layers : This is the solution could be used in both, scenarios (exploding and vanishing gradient). However, by reducing the amount of layers in our network, we give up some of our models complexity, since having more layers makes the networks more capable of representing complex mappings. <br>\n",
    "B) Proper Weight Initilization : A more careful initialization choice of the random initialization for your network tends to be a partial solution, since it does not solve the problem completely. <br>\n",
    "C) Decrease the learning rate to solve Exploding Gradient Problem (deeper layers are not being trained too much since the gradient is small and the learning rate is smaller we have to train the NN for more epochs to make these layers learn while layers close to input layers will get a smaller change (improvement) since we use a lower learning rate OR increase the learning rate if we have the problem of vanishing gradient problem (small updates). <br>\n",
    "D) Batch Normalization : By standardizing the activations values (inputs of previous layer) it makes sure to have uniform distribution of gradients on all weights.\n",
    "E) Gradient Clipping : Checking for and limiting the size of the gradients whilst our model trains is another solution\n",
    "\n",
    "<br>\n",
    "Reference : https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Why does a standard ANN not work for sequential data (for example text data) ?\n",
    "1. Doesnt share features learned across different positions of text. In sequential data the position is important (whether a word appears in the first or third position for example).\n",
    "2. Inputs, outputs can be different lengths in different examples. <br>\n",
    "RNN, GRU, LSTM etc solve these kind of problems since the layers and states share features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Explain how uni and bidirectional RNN works.\n",
    "\n",
    "* Unidirectional RNN computes activations for each timestamp based on the input of timestamp t (for example each token/word of the text data) and computed activation from timestamp t-1 (which carries on information about all earlier states from t=1 till t-1). Based on these activations we compute the prediction at each timestamp t. Instead of standard RNN we could also use LSTMs or GRUs blocks. The disadvantage of unidirectional RNN is that the prediction at the current timestamp uses information only from earlier stages in the sequence and ignores later stages. Thats why in most of the cases bidirectional RNN is used. For example : a) He said Teddy Roosevelt was a great President. b) He said Teddy bears are on sale. To predict the word at position 4 we need information from earlier and later states in the sequence. <br>\n",
    "* Bidirectional RNN we first compute activations to go from left to right and then other activations to go from right to left ( this is how forward propagation works; not to be confused with backward propagation). Then we compute the predictions at each timestamp using both activations. Instead of standard RNN we could also use LSTMs or GRUs blocks.\n",
    "* In Deep RNN we use many layers (not only 1 layer like other RNNs). Each activation of each layer is computed based on the activations computed on the lower state and left state. Usually at most 3 layers are used when building Deep RNNs due to its complexity. Usually in the last layer we add some fully connected layers to each timestamp in order to reach a better performance. Deep RNNs are considered to be very complex and need much more time to train. In general instead of RNNs, LSTMs are used because of short term memory problem und bidirectional model to take into account more information. So Deep Bidirectional LSTMs would be a good idea to use. <br>\n",
    " <br>\n",
    "This is how the Forward Propagation of Unidirectional RNN/GRU/LSTM works : \n",
    "<img src='images/img1.png' width=\"600\" height=\"600\"> <br>\n",
    "<img src='images/img5.png' width=\"600\" height=\"600\"> <br>\n",
    "This is how the Forward Propagation of Bidirectional RNN/GRU/LSTM works :\n",
    "<img src='images/img3.png' width=\"600\" height=\"600\"> <br>\n",
    "This is how the Forward Propagation of Deep RNN/GRU/LSTM works : (can also use deep bidirectional) \n",
    "<img src='images/img4.png' width=\"600\" height=\"600\"> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Why expecially RNNs suffer from Vanishing Gradient Problem and why LSTM, GRU solve it?\n",
    "\n",
    "\n",
    "<b>Why using LSTM, GRU ?</b> <br> \n",
    "Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So if you are trying to process a paragraph of text to do predictions for example predicting the next word, RNN’s may leave out important information from the beginning. But what leads to the short term memory problem in RNN ? During back propagation, recurrent neural networks suffer from the vanishing gradient problem. Gradients are values used to update a neural networks weights. The vanishing gradient problem is when the gradient shrinks as it back propagates through time. Since especially with very long sequences we would have many RNNs blocks, a gradient value becomes extremely small (since to compute the entire gradient we multiply in each RNN block by the gradient of each activation function whose value is always between 0 and 1), it doesn’t contribute too much learning. So in recurrent neural networks, due to these layers that get a small gradient, update stops learning. Those are usually the earlier layers since for the gradient of weights on these layers becomes smaller and smaller. So because these layers don’t learn, RNN’s can forget what it seen in longer sequences, thus having a short-term memory. LSTM’s and GRU’s were created as a method to mitigate short-term memory using mechanisms called gates. Gates are just neural networks that regulate the flow of information flowing through the sequence chain . LSTM’s and GRU’s are used in state of the art deep learning applications like speech recognition, speech synthesis, natural language understanding, etc. <br>\n",
    "\n",
    "Ff we want build a RNN which predicts the next word based on some previous words or sentences. \n",
    "Some words like \"the\", \"with\", \"a\", \"at\" do not give relevant information about the prediction while some other words might play a important role. LSTM or GRU can learn to keep only relevant information to make predictions, and forget non relevant data.\n",
    "\n",
    "<b>Comparison of LSTM and GRU</b> <br>\n",
    "A) The key difference between GRU and LSTM is that GRU's bag has two gates that are reset and update while LSTM has three gates that are input, output, forget. GRU is less complex than LSTM because it has less number of gates. As a result LSTMs should in theory remember longer sequences than GRUs and outperform them in tasks requiring modeling long-distance relations. <br>\n",
    "\n",
    "B) GRUs train faster and perform better than LSTMs on less training data if you are doing language modeling since it has less number of gates.<br>\n",
    "\n",
    "C)  since they are more complax..\n",
    "\n",
    "\n",
    "<br>\n",
    "Reference : https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is the difference between Mini-Batch, Batch and Gradient Descent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Batch Gradient Descent <br>\n",
    "In Batch Gradient Descent, all the training data is taken into consideration to take a single step in Gradient Descent loop. We calculate the partial derivatives of cost function with respect to each parameter (gradients) using the entire training examples and then use that gradient to update our parameters.  <br>\n",
    "<b>Advantages:</b> <br>\n",
    "The cost keeps on decreasing over the epochs and we move somewhat directly and quite smoothly towards an optimum solution since when backpropagating the model learns from the entire dataset.<br>\n",
    "<b>Disadvantages:</b> <br>\n",
    "In Deep learning the datasets are usually huge.Suppose our dataset has 5 million examples, then just to take one step/update parameters in the Gradient Descent Loop the model will have to calculate the gradients of all the 5 million examples. So parameters updates are done very slowly. This does not seem an efficient way.\n",
    "2. Stochastic Gradient Descent\n",
    "In Stochastic Gradient Descent (SGD), we consider just one example at a time when calculating the gradients. <br>\n",
    "<b>Advantages:</b> <br>\n",
    "It is very fast since it considers only one single example to compute the gradients. As a result we ll have many parameters updates. <br>\n",
    "<b>Disadvantages:</b> <br>\n",
    "Since it considers only one training example from the dataset to compute the gradient during backpropagation, the loss/cost function can increase or decrease after parameters update (over time). But in the long run, you will see the cost decreasing with fluctuations. Also because the cost is so fluctuating, it will never reach the minimum but it will keep dancing around it.\n",
    "3. Mini Batch Gradient Descent <br>\n",
    "In Mini Batch Gradient Descent we consider k examplen from the dataset (batch) at a time when calculating the gradients.\n",
    "Doing this helps us achieve the advantages of both the former variants we saw. Thats why this is the most used technique for Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is Transfer Learning and its advantages ?\n",
    "Transfer learning is the reuse of a pre-trained model (that was trained on a specific task with usually a huge dataset) on a new problem/task (similar one). We usually use the weights already learnt by this model to further optimize them (fine-tune the pre-trained model) based on our specific task/goal. Some pre-training in NLP tasks are : language modelling (next word), masked words, next sentence. <br>\n",
    "<b>Desired Goals or Advantages :</b> <br>\n",
    "* Reduce training time. By using a pre-trained model we further optimize the already learnt weights, so we ll have faster convergence.\n",
    "* Improve prediction. We can reach better model performance because we use a model that was trained on another task and with lots of data which might be helpful.\n",
    "* Small dataset. We use only small dataset to fine-tune the pretrained model because it was already trained on a huge dataset so we dont need to fine-tune using another huge dataset.\n",
    "\n",
    "Transfer Learning can be done in two ways : 1) Feature Based, 2) Fine-Tuning <br>\n",
    "A) Feature Based means that we use pre-trained word embeddings from another model and use them to train our model on a downstream task and make predictions. We train the entire model on our task from scratch apart from word embeddings. (usually with Transformers) <br>\n",
    "B) Fine-Tuning. We get the already learnt weights by another pre-trained, usually add one or more fully connected layers into our NN. We either fix those pre-trained weights and only train our added layers or retrain the entire weights by initializing them with the pretrained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What are activation functions ?\n",
    "\n",
    "Activation function are function used in Neural Networks in to introduce nonlinearity in the network. The aim of activation function is to capture non-linear and complex relationship between the features. If Neural Network would have no activation functions (but just the linear relationship between weights and features) it would be just as simple as a LinearRegression Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - When does Gradient Descent stop ?\n",
    "1. We usually specify a limit nr of iterations. \n",
    "2. If the gradient of cost function becomes 0, we reached the local min. => no weight update/weights will remain the same. This is not the case in practical usage of NN since we usually never reach exactly the minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How does learning rate impact the NN performance ?\n",
    "Learning Rate is an important hyperparameter in Gradient Descent. Its value determines how fast the Neural Network would converge to minima/we just take a part of the derivative/gradient of cost function.\n",
    "Usually, we choose a learning rate and depending on the results change its value to get the optimal value for LR. If the learning rate is too low for the Neural Network the process of convergence would be very slow and if it’s too high the converging would be fast but there is a chance that the loss might overshoot or we even diverge. So we usually tune our parameters to find the best value for the learning rate. But is there a way we can improve this process? <br>\n",
    "<b>Why adjust Learning Rate?</b><br>\n",
    "Instead of taking a constant learning rate, we can start with a higher value of LR and then keep decreasing its value periodically after certain iterations. This way we can initially have faster convergence whilst reducing the chances of overshooting the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How does Weight Initialization affect the NN's performance ?\n",
    "\n",
    "* Different Weights Initializations may lead to different local minima. If we are lucky we reach a \"good\" minimum. We should keep in mind that if we dont set a seed (random state) we ll get different results.\n",
    "* If weights are too large or too small may lead to Vanishing or Explodnig Gradient Problem\n",
    "* If the weights are the same the model will not be learning since gradients will be the same.\n",
    "\n",
    "Reference : https://www.analyticsvidhya.com/blog/2021/05/how-to-initialize-weights-in-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is Batch Normalization ?\n",
    "\n",
    "Through adding extra layers in a deep neural network we face the problem of vanishing and exploding gradient. Batch normalization, it is a process to make neural networks faster and more stable through adding extra layers in a deep neural network. The new layer performs the standardizing and normalizing operations on the input of a layer coming from a previous layer (we standardize the activation values). This will lead to a uniform distribution of the gradients (errors) when we backpropagate. (we will not have huge gradients in some layers and smalll gradients in some other layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference : https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How many parameters does a NN have ?\n",
    "\n",
    "* NN has a matrix of weights/parameters for each layer (except the input layer) : the matrix for a particular layer i has as many rows as the number of neurons in the layer i and as many columns as the number of neurons in the previous layer (why: since activation/output of previous layer works as input for the next layer). So layer i would have nr_rows * nr_columns weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Say we remove a decision tree from a Boosting Alg like Random Forest and Bagging Alg like XGBoost. Which alg'performance would be more affected ?\n",
    "* Random Forest in composed of n Decision Trees which are trained independently. It averages the result of all decision tree. If we take out just 1 out of n, it would not affect its performance that much as long as we do not remove a Decision Tree which makes some predictions (outlier) that are quite diffferent from the others.\n",
    "* Since Boosting the Deciison Tree are connected in a sequence and depend on each other it would highly affect the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Why do we use a learning rate in Gradient Descent ?\n",
    "\n",
    "If we update our weights using only the gradients (without the learning rate) it may happen that the gradients are too high (exploding gradient) and we update the weights such that we miss the local minima. Using the leearning rate (<1), we just take a small part of the gradient into consideration to make smaller steps into the local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
