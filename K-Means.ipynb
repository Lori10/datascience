{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e468a325",
   "metadata": {},
   "source": [
    "### - What is Clustering ?\n",
    "Clustering is one of the most common exploratory data analysis technique used to get an intuition about the structure of the data. It can be defined as the task of identifying subgroups in the data such that data points in the same subgroup (cluster) are very similar while data points in different clusters are very different. In other words, we try to find homogeneous subgroups within the data such that data points in each cluster are as similar as possible according to a similarity measure such as euclidean-based distance or correlation-based distance. The decision of which similarity measure to use is application-specific. <br>\n",
    "Clustering analysis can be done on the basis of features where we try to find subgroups of samples based on features or on the basis of samples where we try to find subgroups of features based on samples. We‚Äôll cover here clustering based on features. Clustering is used in market segmentation; where we try to find customers that are similar to each other whether in terms of behaviors or attributes, image segmentation/compression; where we try to group similar regions together, document clustering based on topics, etc. <br>\n",
    "Unlike supervised learning, clustering is considered an unsupervised learning method since we don‚Äôt have the ground truth to compare the output of the clustering algorithm to the true labels to evaluate its performance. We only want to try to investigate the structure of the data by grouping the data points into distinct subgroups. We will cover only Kmeans which is considered as one of the most used clustering algorithms due to its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2408b48",
   "metadata": {},
   "source": [
    "### - What is K-Means and how it works ?\n",
    "Kmeans algorithm is an iterative algorithm that tries to partition the dataset into Kpre-defined distinct non-overlapping subgroups (clusters) where each data point belongs to only one group. It tries to make the intra-cluster data points as similar as possible while also keeping the clusters as different (far) as possible. It assigns data points to a cluster such that the sum of the squared distance between the data points and the cluster‚Äôs centroid (arithmetic mean of all the data points that belong to that cluster) is at the minimum. The less variation we have within clusters, the more homogeneous (similar) the data  are within cluster.\n",
    "* Each cluster representative is its center of mass (i.e., centroid) <br>\n",
    "* The centroid of a cluster is the mean of the instances assigned to that cluster. (if we have n-dimensional data points we calculate the mean of each dimension/feature) \n",
    "\n",
    "How does K-Mean Algorithm work?\n",
    "1. Specify the number of output clusters K\n",
    "2. Select K observations at random from the N data points as the initial cluster centroid\n",
    "3. Assignment step:Assign each observation to its closest centroid based on \n",
    "    the distance measure chosen. (Monotonically decreases SSD)\n",
    "4. Update step: For each of the K clusters update the centroid by computing the \n",
    "    new mean values of all the data points now in the cluster. (SSD Optimization)\n",
    "5. Iteratively repeat steps 3-4 until a stopping criterion is met <br>\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ce8b4b",
   "metadata": {},
   "source": [
    "### - How to choose the stoping criteria ?\n",
    "Several options to choose from:\n",
    "‚Ä¢ Fixed number of iterations\n",
    "‚Ä¢ Cluster assignments stop changing (beyond some threshold)\n",
    "‚Ä¢ Centroid doesn't change (beyond some threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf7327",
   "metadata": {},
   "source": [
    "### - K-Means Complexity Analysis\n",
    "K - nr of clusters, N - nr of data points, d - nr of features <br>\n",
    "* Computing the distance between two d-dimensional data points takes\n",
    "O(d)\n",
    "* (Re-)Assigning clusters [E-step]: O(KN) distance computations or \n",
    "O(KNd)\n",
    "* Computing centroids [M-step]: O(Nd) as there are O(N) average\n",
    "computations since each data point is added to a cluster exactly once at\n",
    "each iteration, each one taking O(d)\n",
    "* Overall: O(RKNd) assuming the 2 steps above are repeated R time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16568a3",
   "metadata": {},
   "source": [
    "### - How does KMeans++ work and compare it to 'Vanilla' KMeans ?\n",
    "* A preliminary method to carefully select initial centroids proposed in \n",
    "2007 by Arthur and Vassilvitskii [paper]. Intuition: spreading out the K initial cluster centers is a good thing\n",
    "1. Choose one center uniformly at random from among initial data points\n",
    "2. For each data point x, compute D(x) as the distance between x and the \n",
    "nearest center that has already been chosen (euclidan distance)\n",
    "3. Choose one new data point at random as a new center with probability\n",
    "proportional to D(x)^2.\n",
    "4. Repeat steps 2. and 3. until K centers are chosen, then run Kmeans-Forg <br>\n",
    "\n",
    "<b>\"Vanilla\" K-means vs. K-means++</b>\n",
    "* Random initialization used with \"vanilla\" K-means may produce clusters \n",
    "that are arbitrarily worse than optimum\n",
    "* K-means++ provides an upper-bound to the approximation obtained w.r.t. the optimal solution\n",
    "* At most, clusters obtained with K-means++ initialization are O(log K)  worse than the optimal partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0904f",
   "metadata": {},
   "source": [
    "### - Why use Random Sampling instead of argmax when choosing the centroids ?\n",
    "\n",
    "If we have outliers and we choose the centroid with the highest probability we would end up choosing outliers as centroid (since their distance to other points is much larger) which is not a good choice as long as we are not performing clustering for outlier detection. (when performing outlier detection it might be a good choice to select an outlier as a centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a954e75b",
   "metadata": {},
   "source": [
    "### - How to select the nr of clusters ?\n",
    "\n",
    "1. Elbow Method\n",
    "\n",
    "* We must select nr of clusters before running KMeans\n",
    "* Unfortunately, it is very uncommon to know K in advance Finding the ‚Äúright‚Äù number K of clusters is part of the problem!\n",
    "* Trade-off between having too few and too many clusters\n",
    "* Elbow Method is empirical method to figure out the right number K of clusters. \n",
    "* Try multiple values of K and look at the change of the SSD using Elbow method. SSD is the sum of squared distances of all points to their corresponding cluster.\n",
    "* As K increases, SSD sharply decreases.\n",
    "* In general we want SSD to decrease but not always since for example if we increase the nr of clusters to a huge nr we ll start creating small clusters inside a optimal cluster.\n",
    "* We take advantage of big decreases in SSD (good optimization) and stop when SSD increases very slowly (bad optimization; this is the step where we create those small clusters).\n",
    "* In the first graph the dataset is clear (SSD decreases drastically when increasing nr clusters from 1-3 and then very slowly) and it is obvious that the nr of optimal clusters is 3 but in the second graph the data is very unclear since the SSD decreases uniformly\n",
    "* Note that such curves are often scale invariant, i.e. e.g. the area between 100 and 200 with an adjusted Y-axis (approx. 60-70) would again have its own elbow to have. That means you need a basic assumption regarding the order of magnitude of k.\n",
    "<img src=\"images/elbow.png\" alt=\"\" width=700 height=700 /> <br>\n",
    "\n",
    "2. Cross Validation\n",
    "3. The \"correct\" number of clusters can ultimately only be determined by analyzing the Clusters in the generated clustering model based on metrics and the required domain knowledge can be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa45d513",
   "metadata": {},
   "source": [
    "### - The elbow method is based on the observation that an increase in Cluster number k (almost!) always leads to a reduction of the quadratic Error ùê∏ùëüùëü2 leads ‚Äì see above ‚Äì why is that so (and why not always\n",
    "\n",
    "For each nr of clusters we compute SSD(error). If we are unlucky and have a bad initilization we would end up having a huge SSD which might be higher than SSD of lower nr of clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1562e2",
   "metadata": {},
   "source": [
    "## - Which factors affect the output of KMeans ?\n",
    "\n",
    "- Initialization of centroids\n",
    "- Nr of clusters we choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0b1174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
