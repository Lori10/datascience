{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Learn Machine Learning Algorithms For Interviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How it works ?\n",
    "\n",
    "It is called Bayes because it uses the Bayes Theorem : P(A/B) = P(B/A) * P(A) / P(B). It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features (independet features are not correlated). Hence each feature individually contributes to identify that it is an apple without depending on each other. We do not use any optimization to create Naive Bayes model; instead we just calculate the frequencies and probabilities).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does it make prediction ? \n",
    "The representation for naive Bayes is probabilities.\n",
    "\n",
    "A list of probabilities are stored to file for a learned naive Bayes model. This includes: <br>\n",
    "**A) Class Probabilities**: The probabilities of each class in the training dataset (given a tweet what is the probability of being positive or negative). For example : P(class=1) = count(class=1) / (count(class=0) + count(class=1))<br>\n",
    "**B) Conditional Probabilities** : The conditional probabilities of each input value/token/word given each class value. We calculate them by dividing the nr of times that a token/word appears in positive bzw negative tweets with the total nr of tokens/words in positive bzw negative tweets P(token/class=1) = count(token in class 1 tweets) / count(all tokens in class 1). Tokens with probability P(token/class=1) > P(token/class=0) means that they show more positive sentiment while tokens with probability P(token/class=0) > P(token/class=1) means that they show more negative sentiment.<br>\n",
    "**C) Normalized Class Probabilities**: For each token/word we calculate the probability = P(word/class=1) / P(word/class=0). For a given word a probability higher than 1 (P(token/class=1 > P(token/class=0)) means it gives more positive sentiment than negative sentiment.\n",
    "\n",
    "Y_i = argmax P(Y=i / word1 ... wordn) = argmax P(Y=i) * (P(word1 / Y=i) * ... * P(wordn / Y=i))  where class_i = [0.1] and word1 ... wordn are the words in the tweet. So we choose the class that has the highest probability given tweet word1...wordn .\n",
    "<img src='images/F.jpeg' width=\"400\" height=\"400\"> <br>\n",
    "\n",
    "To avoid calculate the probabilities for each class we can only calculate single probability and therefore simplify the max objective : <br>\n",
    "Given word1 ... wordn (tweet) : If : (P(word1/class=1) / P(word1/class=0)) * (P(word2/class=1) / P(word2/class=0)) * ... * P(last_word of (tweet/class=1) / P(last word of tweet/class=0)) > 1 : Tweet is positive (1), otherwise the tweet is negative (0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are the Assumptions?\n",
    "Independence : Naive Bayes assumes that independet features are not correlated with each other. This is a very strong assumption that is most unlikely in real data, i.e. that the attributes do not interact because in sentences there is usually a relationship between the words (that is the reason why Naive Bayes is not usually used in real life problems). Nevertheless, the approach performs surprisingly well on data where this assumption does hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Advantages and Disadvantages \n",
    "### Advantages\n",
    "1. Work Very well with many number of features\n",
    "2. Works Well with Large training Dataset\n",
    "3. It converges faster when we are training the model\n",
    "4. It also performs well with categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "1. If independet features are Correlated Naive Bayes may give a low performance. It can not learn the relationship betweent the feautures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Whether Feature Scaling is required?\n",
    "No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Impact of Missing Values?\n",
    "Naive Bayes can handle missing data. Attributes are handled separately by the algorithm at both model construction time and prediction time. As such, if a data instance has a missing value for an attribute, it can be ignored while preparing the model, and ignored when a probability is calculated for a class value\n",
    "tutorial :https://www.youtube.com/watch?v=EqjyLfpv5oA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Impact of outliers?\n",
    "**It is usually robust to outliers.** <br>\n",
    "A model is sensitive to outliers : this means a single bad prediction would ruin the entire model's predicting abilities because the model is going to penalize too high the cost function (high increase in cost function) because of the errors caused by outliers. Se we would end tup choosing the model with lowest cost function which means the model has best fitted to the outliers. <br>\n",
    "A model is robust to outliers : this means erros caused by outliers are not going to penalize (increase so much) the cost function will not get affected too much which means the model is going to ignore erros from outliers and will end up choosing a model which fits the other part of dataset (exluding outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.  Different Problem statement you can solve using Naive Baye's\n",
    "1. Sentiment Analysis (Tweet Sentiment Analysis)\n",
    "2. Spam classification\n",
    "3. document categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. There are three types of Naive Bayes Model, which are given below:\n",
    "\n",
    "**Gaussian**: The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution. <br>\n",
    "**Multinomial**: The Multinomial Naïve Bayes classifier is used when the data is multinomial distributed. It is primarily used for document classification problems, it means a particular document belongs to which category such as Sports, Politics, education, etc.\n",
    "The classifier uses the frequency of words for the predictors. <br>\n",
    "**Bernoulli**: The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
