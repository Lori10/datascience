{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Learn Machine Learning Algorithms For Interviews\n",
    "\n",
    "#### SVM\n",
    "\n",
    "Theoretical Understanding:\n",
    "\n",
    "1. https://www.youtube.com/watch?v=H9yACitf-KM\n",
    "2. https://www.youtube.com/watch?v=Js3GLb1xPhc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Whats the kernel trick and how is it useful ? \n",
    "The kernel trick involves kernel functions that can enable in higher dimension spaces without explicitly calculating coordinates of points within that dimension; instead, kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high dimensional space with lower dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. How it works ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What Are the assumption?\n",
    "There are no such assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Advantages und Disadvantages\n",
    "### Advantages\n",
    "1. SVM is more effective in high dimensional spaces.\n",
    "2. SVM is relatively memory efficient.\n",
    "3. SVMâ€™s are very good when we have no idea on the data.\n",
    "4. Works well with even unstructured and semi structured data like text, Images and trees.\n",
    "5. The kernel trick is real strength of SVM. With an appropriate kernel function, we can solve any complex problem.\n",
    "6. SVM models have generalization in practice, the risk of over-fitting is less in SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "1. More Training Time is required for larger dataset\n",
    "2. It is difficult to choose a good kernel function\n",
    "https://www.youtube.com/watch?v=mTyT-oHoivA\n",
    "3. The SVM hyper parameters are Cost -C and gamma. It is not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Whether Feature Scaling is required?\n",
    "Yes\n",
    "### 6. Impact of Missing Values?\n",
    "Although SVMs are an attractive option when constructing a classifier, SVMs do not easily accommodate missing covariate information. Similar to other prediction and classification methods, in-attention to missing data when constructing an SVM can impact the accuracy and utility of the resulting classifier.\n",
    "### 7. Impact of outliers?\n",
    "It is usually sensitive to outliers\n",
    "https://arxiv.org/abs/1409.0934#:~:text=Despite%20its%20popularity%2C%20SVM%20has,causes%20the%20sensitivity%20to%20outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Overfitting And Underfitting\n",
    "In SVM, to avoid overfitting, we choose a Soft Margin, instead of a Hard one i.e. we let some data points enter our margin intentionally (but we still penalize it) so that our classifier don't overfit on our training sample\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Different Problem statement you can solve using SVM\n",
    "1. We can use SVM with every ANN usecases\n",
    "2. Intrusion Detection\n",
    "3. Handwriting Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Whats the kernel trick and how is it useful ? \n",
    "The kernel trick involves kernel functions that can enable in higher dimension spaces without explicitly calculating coordinates of points within that dimension; instead, kernel functions compute the inner products between the images of all pairs of data in a feature space. This allows them the very useful attribute of calculating the coordinates of higher dimensions while being computationally cheaper than the explicit calculation of said coordinates. Many algorithms can be expressed in terms of inner products. Using the kernel trick enables us effectively run algorithms in a high dimensional space with lower dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
