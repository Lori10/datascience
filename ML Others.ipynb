{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is dimensionality reduction and why it is used ?\n",
    "Dimensionality reduction techniques can be divided into 2 parts : feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - When performing features scaling, why should we scale between 0 and -1 (normalization) or -3 and 3 (standardization) and not for example between 1 and 1000 ?\n",
    "We scale to a range between -3 and 3 in order to have a better understanding of the model interpretation. If we have a feature that ranges between 1 and 1000 and has for example a standard deviation of 20; suppose after training its weight is 2; we say that if the feature increases by 1 the target increases by 2 but 1 unit increase in the feature is quite small compared to its standard deviation or scale. In case of scaling between -3 or 3 (suppose standard deviation = 3, mean=0), 1 unit increase is relatively high increase and we can better interpret how this feature affects the target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is the difference between discriminative and generative classifiers ?\n",
    "There are two types of Supervised Learning algorithms used for classification in Machine Learning :\n",
    "\n",
    "1. Discriminative Learning Algorithms. \n",
    "2. Generative Learning Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Discriminative Learning Algorithms include Logistic Regression, Perceptron Algorithm, etc. which try to find a decision boundary between different classes during the learning process. For example, given a classification problem to predict whether a patient has malaria or not a Discriminative Learning Algorithm will try to create a classification boundary to separate two types of patients, and when a new example is introduced it is checked on which side of the boundary the example lies to classify it based on the threshold that we choose. Such algorithms try to model P(y=1|X) i.e. given a feature set X for a data sample what is the probability it belongs to the class ‘y’=1.\n",
    "2. On the other hand, Generative Learning Algorithms follow a different approach, they try to capture the distribution of each class separately instead of finding a decision boundary among classes. Considering the previous example, a Generative Learning Algorithm will look at the distribution of infected patients and healthy patients separately and try to learn each of the distribution’s features separately, when a new example is introduced it is compared to both the distributions, the class to which the data example resembles the most will be assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is curse of dimensionality ?\n",
    "It is observed that on increasing the number of features the accuracy tends to increase until a certain threshold value and after that, it starts to decrease.\n",
    "the driver’s height, hair color, car color, car company, and the driver’s marital status is giving useless information for the model to learn, hence the model gets confused with all this extra information, and the accuracy starts to go down.\n",
    "In general if we keep adding important features (feature that give useful information about the target) the accuracy increases. Gut if we add features that do not give any useful information about the target and are not correlated with it in any kind of relationship, model gets confused with all this extra information, and the accuracy starts to go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is overfitting and underfitting and the trade-off between these two ?\n",
    "* The model is overfitting to the training data if it fits the training data very well/with a very high performance which means fits even the noisy data (model learns expect the general trend even some specific uncommon trends. Since it fits the trainind data so well it will fail to generalize for new data because it failed to capture the general trend. An overfitted model has high variance and low bias which means training error is low and test error is high.\n",
    "* The model is underfitting if the model fails to capture the general trend in the training data/does not fit well the traiing data. An underfitted model has high bias and low variance whic means training and test error are both high but test error might be lower than training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - How to avoid overfitting ? \n",
    "1. Collect more data\n",
    "2. Try ensemble methods since they generalize well\n",
    "3. Try a simpler model rather than a complex one (for example in ML reducing nr of features using feature selection or using feature importance chart since there might be feature that does not give any useful information about the target feature, decresing the polynomial degree, in DL reducing nr of neurons, nr of layers)\n",
    "4. Regularization Techniques (L1, L2)\n",
    "5. Dropout (used in Deep Learning)\n",
    "6. Early Stopping\n",
    "7. Hyperparameter Tuning. Chosing the right values for hyperparameters that can avoid model overfitting for example max_depth, n_estimators using cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Compare Gradient Descent with Normal Equation of Ordinary Least Squared (OLS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Gradient Descent needs to choose alpha (hyperparameter) but OLS does not need to set any hyperparameter.\n",
    "2. Gradient Descent is an iterative optimization process while OLS is just a matrixes multiplication.\n",
    "3. OLS is slower than GD when the nr of features is too large since inverse(t(X) * X) needs too much time to be computed.\n",
    "4. OLS requires to compute (XTX)−1, which have rare cases that matrix is non-invertible, so you may have to reduce dimensions or avoid multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Types of CrossValidation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a statistical method used to estimate the performance (or accuracy) of machine learning models. It is used to protect against overfitting in a predictive model, particularly in a case where the amount of data may be limited. In cross-validation, you make a fixed number of folds (or partitions) of the data, run the analysis on each fold, and then average the overall error estimate.\n",
    "1. Holdout.\n",
    "We split only in training und test set. In this approach, the data is first shuffled randomly before splitting. As the model is trained on a different combination of data points, the model can give different results every time we train it, and this can be a cause of instability. Also, we can never assure that the train set we picked is representative of the whole dataset. The hold-out method is good to use when you have a very large dataset, you’re on a time crunch, or you are starting to build an initial model in your data science project.\n",
    "2. K-fold cross validation is one way to improve the holdout method. This method guarantees that the score of our model does not depend on the way we picked the train and test set and the result we get from this method is more reliable. Because it ensures that every observation from the original dataset has the chance of appearing in training and test set, this method generally results in a less biased model compare to other methods. It is one of the best approaches if we have limited input data. The disadvantage of this method is that the training algorithm has to be rerun from scratch k times, which means it takes k times as much computation to make an evaluation.\n",
    "Steps : a) Randomly split your entire dataset into k number of folds (subsets)\n",
    "b) For each fold in your dataset, build your model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n",
    "c) Repeat this until each of the k-folds has served as the test set\n",
    "d) The average of your k recorded accuracy is called the cross-validation accuracy and will serve as your performance metric for the model.\n",
    "4. Stratified K Fold CV : Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the data such that in every fold, each class comprises of about half the instances.\n",
    "4. Leave One Out CV : This is a simple variation of Leave-P-Out cross validation and the value of k is set to n (number of rows). The computational cost is quite huge and The model may lead to a low bias (overfitting) since we use too much data for training and very less data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
