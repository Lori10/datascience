{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "861e9194",
   "metadata": {},
   "source": [
    "### - Which are the 2 types of statistics ?\n",
    "Statistics is mainly divided into the following two categories: 1. Descriptive Statistics and 2. Inferential Statistics.\n",
    "1. Descriptive Statisticks\n",
    "In the descriptive Statistics, the Data is described in a summarized way. The summarization is done from the sample of the population using different parameters like Measure of central tendency : Mean, median, mode and measure of variability : variance, standard deviation, range or quantiles etc. Descriptive Statistics are a way of using charts, graphs, and summary measures to organize, represent, and explain a set of Data. Data is typically arranged and displayed in tables or graphs summarizing details such as histograms, pie charts, bars or scatter plots. Descriptive Statistics are just descriptive and thus do not require normalization beyond the Data collected.\n",
    "2. Inferential Statistics :\n",
    "Inferential Statistics makes inference and prediction about population based on a sample of data taken from population. It generalizes a large dataset and applies probabilities to draw a conclusion. It is simply used for explaining meaning of descriptive stats. It is simply used to analyze, interpret result, and draw conclusion. \n",
    "We take samples from the population since it generally is impossible to collect information of the entire population. <br>\n",
    "In statistics there is the true but unknown distribution (population) for example age of people (X which is random variable in probability theory) in Germany. This distribution (if its one the distribution family like normal or exponentiall distribution; we know the parameters that define these types of distributions we already know the entire distribution of the data) is defined by some specific parameters like expected value, variance, lambda etc (If its not any of the distribution families we are interested only on its expected value and variance. We take a sample from this population and try to estimate/predict for example the expected value/variance of population. For example the mean of our sample would be a point estimate of the population mean and sample variance a point estimate of the population variance. Inferential Statistics is mainly related to and associated with hypothesis testing whose main target is to reject null hypothesis so we usually perform hypothesis test about these unknown parameters of the population distribution and compute confidence intervalls about them using our sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ea2ed4",
   "metadata": {},
   "source": [
    "### -  How to check whether a linear depedence (correlation) exists between 2 features and how strong this correlation this?\n",
    "1. When we talk about linear dependence, we mean the correlation. To check whether 2 features (feature x_i and target y) are correlated or not we perform a t-test with a given significance level (usually 5%). H0 : b_i = 0 (there is no correlation) since the regression coefficient of x_i is equal to 0, H1 : b_i != 0 (there is a correlation). If we get p_value < 0.05 we reject H0 and say that the linear Dependency between 2 features is statistically significant with an error of 5% ; otherwise we fail to reject H_0 (but we dont say that we accept H_0) and can not conclude that the features are correlated based on your data (sample). We could also compute confidence intervalls for the estimated b_o and check if 0 lies in the intervalls or 0 is far from its upper und lower bounderies.\n",
    "\n",
    "Note : we can not just compute correlation coefficient between x_i and y since the correlation coefficient may be high but we can not only rely on it since it may be just a result due to randomness in our data. If we would get more data this correlation coefficient may decrease\n",
    "\n",
    "2. To check the strongness of the correlation, we compute the estimated correlation coefficient using our data. Usually if it is a value greater than 0.07 or lower than -0.07 there is a relative strong correlation. If it is close to 0 the correlation is quite weak. (If we can draw a line throught the points (features) and prefectly cover them the correlation is perfect 1 and we can express it using a regression equation y=a*x_i + b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887728f3",
   "metadata": {},
   "source": [
    "### - How do we generally perform a statistical hypothesis test ?\n",
    "1. Assume that our sample data points (x1, ..xn realisations of our X1,.. Xn random variables) are identical distributed x1, .., xn following an known population distribution. \n",
    "2. Set up the null and alternative hypothesis. For example H_0 : E(X)=10; H_1 : E(X) != 10 (two sided test) OR H_0 : E(X)<10; H_1 : E(X)>=10 (one sided test) OR H_0 : E(X)>10; H_1<=10 (one sided test)\n",
    "3. Set the confidence level, alpha (ð›¼), is used as a threshold to determine whether the null hypothesis should be accepted or rejected. It is also represents the probability that you reject the null hypothesis when it is actually true = Probability of making a Type 1 Error.\n",
    "4. Build and compute the Test-Statistic which is a function of our sample data (therefor is a random variable) and based on which we will decide whether we reject or not the null hypothesis. For example if H_0 : E(X)>10 where X is age population, we compute the mean of our sample as the test statistic. We must know the probability distribution of the Test-Statistic when Null Hypothesis is true. Based on the computed Test-Statistic value and its distribution under H_0 we decide whether we reject or not the null hypothesis. If we get a value which is very unlikely to have a high probability under H_0, it means that H_0 is false and we reject, otherwise we fail to reject it. In our example high values of the Test-Statistic (sample mean) means that null hypothesis is true, otherwise null hypothesis is false (but we have to decide this threshold where we reject H_0 or fail to reject it using alpha)<br>\n",
    "Note : If population variance is known we use the normal distributiin (Test-Statistic is normally distributed), otherwise it is T distributed (T Distribution) OR we assume that the Test-Statistic is approximately normally distributed if sample size is greater than 30 (central limit theorem). Then we standardize the test_statistic in order to easily use quantiles of a know distribution (like normal distr or t distr) to compute the critical values.\n",
    "5. Set the rejection region based on significance level and distribution of test-statistik under H_0. (use the critical values/quantiles). For one-side test we have only 1 intervalls; for two-sided test we can reject in 2 intervalls.\n",
    "6. <b>Traditional Test : </b> If Test Statistic value lies in the rejection regions (compare it with critical values) we reject H_0, otherwise we fail to reject H_0 <br>\n",
    "<b> P Value Test : </b> We compute p-value which is the probability of seeing this Test-Statistic value or a more extrem value under H_0. If p-value is lower than 0.05 (significance level) we reject H_0, otherwise we fail to reject. Both ways lead to the same result.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575b2eef",
   "metadata": {},
   "source": [
    "### - In hypothesis testing explain Type 1 and Type 2 Error ? Why do we put the statement that we want to prove on the alternative H_1 ? How do we interpret a hypothesis test ?\n",
    "* In hypothesis testing, the goal is to determine whether a statement (null hypothesis) is true or false. The alternative hypothesis, H_1, is a statement of what a statistical hypothesis test is set up to establish. In some cases, however, researchers will reject or accept the null hypothesis when they shouldnâ€™t have. Data Scientists refer to these errors as Type I(null hypothesis is true but we reject; False Positive) and Type II (null hypothesis is false but we fail to reject it; False Negative) errors.\n",
    "* The confidence level, alpha (ð›¼), is used as a threshold to determine whether the null hypothesis should be accepted or rejected. It is also represents the probability that you reject the null hypothesis when it is actually true = Probability of making a Type 1 Error). Our aim is always to reject the null hypothesis in order to prove our statement (prove H_1) or what we want to prove we set on the alternative hypothesis. The opposite of it goes to the null hypothesis. The probability of not rejecting the null hypothesis when it is false = Probability of making a Type 2 Error. \n",
    "* We follow this way of setting null and alternative hypothesis since we can control the probability of making a Type 1 error which is equal (or maximal) to the significance level (alpha) and can not control the probability of making the type 2 error (because we do not know the distribution of test statistic under H0). \n",
    "* Interpretation of a hypothesis test : Using our sample data we reject the null hypothesis and accept the alternative hypothesis OR we fail to reject the null hypothesis (but we dont say that we accept the null hypothesis) since we may just not have enough data (small data) that we are using to prove the H_1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2405c9",
   "metadata": {},
   "source": [
    "### - What is p value, power and how to interpret ?\n",
    "\n",
    "* P-Value is the probability of seeing this Test-Statistic value or a more extrem value under H_0. If p-value is lower than 0.05 (significance level) we reject H_0, otherwise we fail to reject. P-Value is the maximal error that we accept. Your p-value is 0.05, that means that 5% of the time (of analyses) you would see a test statistic at least as extreme as the one you found if the null hypothesis was true. The lower the p-value the more significance we have against H0. This is an advantage of p-value since using the traditional statistical test we just decide whether to reject h0 or fail to reject h0 we do not exactly know how much evidence do we have. Usually the more data we have => more significance we have against h0 => the lower p value. We can compare pvalues of two different tests which have the same data size.\n",
    "* Power is the probability of rejecting a null hypothesis when its false or probability of not making Type 2 Error. Formula : Power =  1 - Type 2 Error. The greater the Power the lower the p value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0fc525",
   "metadata": {},
   "source": [
    "### - What are confidence intervalls and how to interpret them ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd0062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f13cca13",
   "metadata": {},
   "source": [
    "### - Explain what kind of statistical tests we use for different purposes.\n",
    "\n",
    "1. <b>One Sample t-test or gauss_test</b>. The goal is to test assumptions about the parameters that define the population distribution.\n",
    "For example the expected value / variance of a population. For example X is the age and we take a sample from this population and build the test hypotheses H_0 : E(X) = 10, H_1 : E(X) != 10. Our aim to reject the H_0. Based on our sample data we perform this test and come to a conclusion. (We could also test other parameters of a distribution for example lambda parameter of exponential distribution)\n",
    "2. <b>Two Sample Independent T-test</b>T is an inferential statistical test that determines whether there is a statistically significant difference between the expected values in two unrelated groups/populations which are supposed to not be depend. It can be used to check whether a discrete and continuos feature are dependent or not. For example we want to check whether the salary (Y) and sex are dependent. For each group of sex (men-X1 and women-X2) we have a sample of salary. H_0 : E(X1) = E(X2), H_1 : E(X1) != E(X2). If the expected values of two groups are the same (fail to reject H_0) it means that sex does not affect the salary. But if for example reject H_0, we say the different groups have different salaries for example men have higher salary (H_0 : E(X1) < E(X2)), then it means that sex and salary are dependent.\n",
    "3. <b>Two Sample Dependent T-Test</b> : It is similary to the independent test. The difference is that in this case we suppose that two groups/populations are dependent. This is often the case when we have pairs of data points for example the health wellnes before and after taking a medicine. We would have data for each person (X,Y) of the health wellness (suppose 0 to 1). X_i and Y_i would be dependent. We build the difference Z=Y-X (or Z=X-Y) and then use the one sample t-test. Make assumptions that the expected value of the difference is equal to 0 (which means that Y is systematically greater than X) H_0 : Z>0, H_1 : Z<=0.\n",
    "4. <b>Independence Test</b> : This test makes use of Chi Squared Distribution. Used to test whether two discrete features are dependent or not. H_O : P(X and Y) = P(X) * P(Y) (features are independent) and H_1 : P(XandY) != P(X) * P(Y) which means features are dependent. \n",
    "5. <b>Goodness of fit</b> : This test makes use of Chi Squared Distribution. It is similar to independence test. It is used to check whether a distribution is same to another expected distribution (can be a known distribution like normal distribution or a given expected distribtuion). Example : H_0 : distibution is the same, H_1 : distribution is not the same.. Example is the Shapirko Wilk Test which assumes that our sample data is normally distributed and H_1 : data is not normally distributed. Goal is to show that the data is normally distributed. Low p values give evidence against H_0 which means data is not normally distributed. Usually higher sample size gives more evidence against H_0. So this test works well if sample is of small size which is not reliable result.\n",
    "6. <b>Test of homogeneity</b> : This test makes use of Chi Squared Distribution. T tests (null hypothesis) whether the distribution of a categorical variable is the same for the populations or subgroups (categorical varible).\n",
    "7. <b>Linear Regression T-Test</b> Check whether 2 continous features (target must be continuous but feature can be categorical) are correlated (linear relationship) or not. H_0 : theta_i = 0 (no correlation), H_1 : theta_i != 0 (correlation exists)\n",
    "8. <b>Wilcoxon test</b> This is a nonparametric test which makes assumption about median of a population instead of mean and does not make the assumpation about normal distribution but assumes that data is symmetric around the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2366d3",
   "metadata": {},
   "source": [
    "### - What kind of estimators are mostly used in Statistic, how to evaluate them and select the best one?\n",
    "\n",
    "The goal is to estimate the parameter of a distribution for example Expected value of population, lambda of exponential distribution, coefficient of linear Regression. Estimators are random variables since for different datasets we get different estimator values. These estimators have expencted value, bias, variance and mse.\n",
    "\n",
    "1. <b>Ordinary Least Squares</b> <br>\n",
    "We define a cost function/error (Sum of Squared) which is a function of the parameter we want to estimate. We try to find the parameters which miminizes the Sum of Squared. We find the optimal value by computing the x (parameter value) where the first derivative is equal to 0 (or all partial derivatives to 0 if we have many variables/parameters) and second derivative must be positive to have minimum or HesseMatrix positiv definint (to have minimum in case of many variables). We do not need to know the distribution of our data.\n",
    "\n",
    "\n",
    "2. <b>Maximum Likelihood Estimator</b> <br>\n",
    "We define and compute the probability of seeing our sample data given a specific value of the parameter we want to estimate (Likelihood Function). Since our data points are independent we set the probability of the entire sample data as product of all single probabilities. The requirement of Maximum Likelihood estimator is that we should know the distribution of our data => how these single probabilities are defined. So we set a Likelihood function as a function of our parameter. We take the log of this function to make the computation easier (set the product as a sum) since log is a monoton function. To find the maximum of the function we use the same way as OLS by making use of first and second derivatives.\n",
    "\n",
    "3. <b>Moments Estimation for Expected Value and Variance</b> <br>\n",
    "We use the mean of a given sample size as an estimation of Expected Value and the empirical variance (divide by n-1 so that the estimator becomes erwartungstreu) as an estimation of the Variance.\n",
    "4. <b> Method of moments </b> <br>\n",
    "This method is used to estimate 2 or more parameters using the Moments Estimaton and Verschiebungssatz [ E(x)^2 = Var(X) + (E(x))^2 ]. <br>\n",
    "\n",
    "* <b>Bias : </b> Estimator est for the unknown but true parameter e is unbiased of Expected E(est)=e or Bias = E(est) - e = 0 ; if with our estimation on average we converge to the true value. To estimate the bias of our estimator we usually take different samples (datas) to compute different estimator values. Then we compute the mean of them as an estimaton for the expected value of the estimator. By subtracting the true value (if known) we can compute an estimaton of the bias.\n",
    "* <b>Variance</b> To compute an estimaton for the Variance of our estimator we can compute the empirical variance from the estimator values we get from the different sample data. It makes sense to plot a histogram of our estimated values to check the variance of our estimator. \n",
    "* <b>MSE (mean squared error) </b> of our estimator is MSE(est) = E((true parameter - estimation)^2) OR MSE(est) = Bias(est)^2 + VARIANCE(est). Empirically we could compute an estimaton for the MSE of our estimator by getting different estimated values for each sample data, subtract from them the true parameter, square and then take the mean. Bias and Variance of the estimator are in trade off (High Variance leads to low bias and low bias to high variance) ; thats why we use MSE. We want to have an estimator with low bias and low variance which means we should select the estimator with lowest MSE. Under all unbiased estimator we select the estimator with lowest variance.\n",
    "* Estimator est is consistent if it converges to the true value of the parameter as the sample size tends to infinity (variance becomes smaller and expected value converges to the true value). Formula : lim when n->infinity of MSE(est) = 0. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc817123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dba488a6",
   "metadata": {},
   "source": [
    "### - What is the difference between correlation coefficient and covariance?\n",
    "* <b>Common Ground:</b> Both Correlation coefficient and covariance determine how strong the <b>linear</b> relationship between 2 random variables is (not any other dependency/relationship of features like quadratic or cubic)\n",
    "* <b>Difference:</b> The difference is that covariance depends on the scale of features but correlation ranges always between -1 and 1. \n",
    "* Covariance Formula : sum( (x_i - x_mean) * (y_i - y_mean )) / n-1. We can see from the formula that it is sensitive to the scale of features x and y.\n",
    "* Correlation Formula : Cov(x,y) / SD(x)*SD(y). As we divide by the product of standard deviations of x and y we always get a value from -1 and 1. Thats why we always use correlation coefficient to check how strong is the liner relationship between 2 features since it is easier to interpret. Values close to -1 and 1 show very strong negative/positive correlation (linear dependency and not any other dependency like quadratic) while values close to 0 is a sign of very weak correlation. Negative Correlation means if x decreases y increases and if x increases y decreases; Positive Correlation means if x increases y increases and if x decreases y decreases. If the correlation coefficient is exactly -1 or 1 we can draw a perfect linear line which shows this relationship.(the closer the points to a linear line the higher the strongness of correlation).\n",
    "* Example : suppose we have features x and y and take log(x) and y. When we take log we would get a lower covariance even though the relationship does not change at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca215da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a22d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38949337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ae317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5380d414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
