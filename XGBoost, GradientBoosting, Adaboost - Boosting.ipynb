{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What Are the Basic Assumption?\n",
    "There are no such assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Missing Values\n",
    "1. Adaboost can handle mising values\n",
    "2. Xgboosst and GBoost cannot handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Advantages and Disadvantages\n",
    "Advantages of Adaboost\n",
    "\n",
    "1. Doesn't Overfit\n",
    "\n",
    "2. It has few parameters to tune\n",
    "    \n",
    "Advantages of Gradient Boost And Xgboost\n",
    "    \n",
    "1. It has a great performance and speed.\n",
    "2. It can solve complex non linear functions \n",
    "3. It is better in solve any kind of ML usecases.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages of Gradient Boosting And Xgboost\n",
    "\n",
    "1.It requires some amount of parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Whether Feature Scaling is required?\n",
    "No because they use Decision Tree internally.\n",
    "\n",
    "### 5. Impact of outliers?\n",
    "Boosted Tree methods should be fairly robust to outliers in the input features since the base learners are tree splits. For example, if the split is x > 3 then 5 and 5,000,000 are treated the same. This may or may not be a good thing, but that's a different question.\n",
    "\n",
    "If instead you were talking about regression and outliers in the target variable, then sensitivity of boosted tree methods would depend on the cost function used. Of course, squared error is sensitive to outliers because the difference is squared and that will highly influence the next tree since boosting attempts to fit the (gradient of the) loss. However, there are more robust error functions that can be used for boosted tree methods like Huber loss and Absolute Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is Boosting Technique ? \n",
    "Boosting is an ensemble approach that starts from a weaker decision/model and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers/models. The weights are assigned based on the performance of an individual tree. Ensemble parameters are calculated in **stagewise way** which means that while calculating the subsequent weight, the learning/errors made from the previous tree is considered as well. This means that it involves several sequential trees; not like RandomForest where each decision tree is trained independetly). Each model learns from the mistakes/missclassification of previos model so this means that each model becomes better in predicting examples from training data (target feature for next model will be the residual of previous model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How XGBoost works ?\n",
    "\n",
    "* XGBoost includes several models (weak models) that are added sequentially where each model learns from the mistakes/missclassification of previos model so this means that each model becomes better in predicting examples from training data (target feature for next model will be the residual of previous model). This means that models are added sequentially until no further improvements can be made. These models are added then together to make the final prediction.\n",
    "* XGBoost the Algorithm operates on decision trees (weak models), models that construct a graph that examines the input under various “if” statements (vertices in the graph). Whether the “if” condition is satisfied influences the next “if” condition and eventual prediction. XGBoost the Algorithm progressively adds more and more “if” conditions to the decision tree to build a stronger model\n",
    "* It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use XGBoost ?\n",
    "The two reasons to use XGBoost are also the two goals of the project:\n",
    "1. Execution Speed. XGBoost is very fast compared to other implementations of gradient boosting.\n",
    "2. Model Performance. It usually outperforms all other ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Types of XGBoost in Scikitlearn \n",
    "The implementation of the model supports the features of the scikit-learn and R implementations, with new additions like regularization. Three main forms of gradient boosting are supported:\n",
    "\n",
    "1. Gradient Boosting algorithm also called gradient boosting machine including the learning rate.\n",
    "2. Stochastic Gradient Boosting with sub-sampling at the row, column and column per split levels.\n",
    "3. Regularized Gradient Boosting with both L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
