{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization  <br>\n",
    "**How it works?**<br>\n",
    "When we use regression models to train some data, there is a good chance that the model will overfit the given training data set for example when in linear regression we have huge values of weights. This means feature with huge weights have high impact on the target feature and model may overfit.  Regularization helps sort this overfitting problem by adding to the cost function that needs to be minimized the regularization term (Shrinkage penalty which penalizes the weigts) which is a positive term. So now we are trying to find parameters thetas/weights that minimizes cost function which in this case is higher in magnitude so we will end up choosing lower values of parameters/weights (some may be close to 0). \n",
    "In a linear equation, model can overfit to the training set when we have huge weights/coefficients as a small change in weight can make a large difference for the dependent variable (Y). So, regularization constraints the weights/impacts of such features to avoid overfitting. <br>\n",
    "\n",
    "**Why use Regularization?** <br>\n",
    "Regularization helps to reduce the variance of the model, without a substantial increase in the bias. If there is variance in the model that means that the model won’t fit well for dataset different that training data. The tuning parameter λ controls this bias and variance tradeoff. When the value of λ is increased up to a certain limit, it reduces the variance without losing any important properties in the data (without underfitting). But if we set it to a huge value/after a certain limit, that would also increase the cost function. The model will start losing some important properties which will increase the bias in the data (it will underfit because it penalizes the weights so much that they will be close to 0. As a result we would have y = 0*x1 + 0*x2 => y = 0) while setting value of lambda to a very small value can still lead to overfitting. Thus, the selection of good value of λ is the key.\n",
    "The value of λ is selected using cross-validation methods. A set of λ is selected and cross-validation error is calculated for each value of λ and that value of λ is selected for which the cross-validation error is minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (Least Absolute Shrinkage and Selection Operator) L1 Regularization\n",
    "\n",
    "LASSO regression penalizes the model based on the sum of magnitude/absolute values of the coefficients. The regularization term is given by\n",
    "\n",
    "regularization = $ \\lambda *\\sum  |\\beta_j| $ Where λ is the regularization parameter. and hence the formula for of cost function after regularization is:\n",
    "\n",
    "<img src=\"images/L1.PNG\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression (L2 Regularization)\n",
    "Ridge regression penalizes the model based on the sum of squares of magnitude of the coefficients. The regularization term is given by regularization=$ \\lambda *\\sum  |\\beta_j ^ 2| $ where λ is the regularization term. and hence the formula for cost function after regularization is:\n",
    "\n",
    "<img src=\"images/ridge.PNG\" width=\"300\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between Ridge and Lasso\n",
    "Ridge regression shrinks the coefficients for those predictors/features which contribute very less in the model but have huge weights, very close to zero. But it never makes them exactly zero. Thus, the final model will still contain all those predictors, though with less weights. This doesn’t help in interpreting the model very well. This is where Lasso regression differs with Ridge regression. In Lasso, the L1 penalty does reduce some coefficients exactly to zero when we use a sufficiently large tuning parameter λ. So, in addition to regularizing, lasso also performs feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net\n",
    "\n",
    "According to the Hands-on Machine Learning book, elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization terms, and you can control the mix ratio α. \n",
    "\n",
    "<img src=\"images/elasticNet.PNG\" width=\"300\">\n",
    "where α is the mixing parameter between ridge (α = 0) and lasso (α = 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When should you use plain Linear Regression (i.e., without any regularization), Ridge, Lasso, or Elastic Net?**\n",
    "\n",
    "According to the Hands-on Machine Learning book, it is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to zero as we have discussed (Lasso and ElasticNet perform feature selection). In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.\n",
    "\n",
    "In Ridge, when we increase the value of LAMBDA, the most important parameters might shrink a little bit and the less important parameter stay at high value. In contrast, with LASSO when we increase the value of LAMBDA the most important parameters shrink a little bit and the less important parameters goes closed to ZERO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is the advantage of Ridge, Lasso compared to Normal Linear Regression ?\n",
    "\n",
    "The closed form solution (matrices) to find the optimal parameter using Normal Linear Regression and the Hat Matrix is : \n",
    "<img src=\"images/linreg.jpg\" width=\"300\"> <br>\n",
    "\n",
    "If NORM(X) != p+1 (Norm of matrix is the number of linear independent columns which means all columns must not be independent which means corr(x_i, x_j) != -1 or 1 or p+1>n => t(X) * X is not convertible => equation does not have unique solution => we have several local minimum.\n",
    "\n",
    "The closed form solution (matrices) to find the optimal parameter using Ridge is : \n",
    "<img src=\"images/equation.PNG\" width=\"300\"> <br>\n",
    "Since we add the term lambda * I the matrix t(X) * X + lambda * I is always convertible and therefore we have only 1 unique solution (1 global minimum)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - What is Hat or Smoothing Matrix ?\n",
    "\n",
    "Hat/Smoothing Matrix includes in each row the weights of all y_i for i=1..n which arae used to predict the predicted_y_i. Suppose we want to predict y_2. If we give a high weight to y_2 and very low weight to the other y_i, the model may overfit since the correlation/covariance(y_i, predicted_y_i) is quite high. The goal is to give each y_i an optimal weight to avoid overfitting. Usually points (y_i) that are closer to the predicted_y_i must have higher weights.\n",
    "<img src=\"images/linreg.jpg\" width=\"300\"> <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
